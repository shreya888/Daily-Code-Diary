# Generative Adversarial Networks Fundamental Concepts
Generative Adversarial Networks (GANs) are a class of machine learning techniques that consist of two networks playing an adversarial game against each other. Both the generator and discriminator start training from scratch, typically initialized with random noise, and train simultaneously. When training begins, the generator produces obviously fake data, and the discriminator quickly learns to tell that it's fake. Finally, if generator training goes well, the discriminator gets worse at telling the difference between real and fake. It starts to classify fake data as real, and its accuracy decreases.
<img src="https://github.com/shreya888/Daily-Code-Diary/assets/25200389/01b0d0ed-2b79-4e67-bf0f-a0adaf643154" width="500">

## Generator Network (G)
* The "Counterfeiter"
* Aim: Generate synthetic data that is indistinguishable from real data by incorporating feedback from the discriminator (make the discriminator classify its output as real)
* The generated instances become negative training examples for the discriminator.
* Training Generator: Sample Random Noise input -> Produce Generator output -> Discriminator classification for gen o/p (Real/Fake) -> Calculate Loss from Discriminator classification -> Backpropagate through both the discriminator and generator to obtain gradients -> Use gradients to change only the generator weights
* Why Random Input and How?
  * NNs need some input. But what should be input to when the objective is to outputs entirely new data instances!?
  * GAN takes random noise as its input. The generator then transforms this noise into a meaningful outputs.
  * By introducing noise, GAN can produce a wide variety of data, sampling from different regions in the target distribution.
  * Experiments suggest that the distribution of the noise doesn't matter much, so we can choose something that's easy to sample from, like a uniform distribution.
  * For convenience the space from which the noise is sampled is usually of smaller dimension than the dimensionality of the output space.
* Using the Discriminator to Train the Generator
  * In traditional neural networks, weights are adjusted to minimize output error or loss.
  * But in GANs the generator is not directly connected to the loss being optimized.
  * The generator feeds into the discriminator, and the discriminator produces the output we're trying to affect.
  * The generator loss penalizes the generator for producing a sample that the discriminator network classifies as fake (penalty for failing counterfeiting). 
  * Backpropagation adjusts each weight in the right direction by calculating the weight's impact on the output — how the output would change if you changed the weight.
  * The impact of a generator weight depends on the impact of the discriminator weights it feeds into. So backpropagation starts at the output and flows back through the discriminator into the generator.
  * Preventing the discriminator to change during generator training is crutial (= hit a moving target - harder problem).

## Discriminator Network (D)
* The "Detective"
* Aim: Distinguish between real and fake data generated by the generator through backpropagation
* Simply a classifier that penalizes the generator for producing implausible results.
* Early in learning, when G is poor, D can reject samples with high confidence because they are clearly different from the training data.

## Loss Functions
GANs utilize two main loss functions, one for each component.

![image](https://github.com/shreya888/Daily-Code-Diary/assets/25200389/6fd5e093-8a46-407e-b0d7-cb007e1fa07d)
* **z**: Generated data sample
* **x**: Real data sample
* **D(x)**: Probability that x came from the data rather than pg (probability of real data to be real)
* **log(D(x))**: Classify if sample came from real or fake distribution; as it real here, it should ideally output 1. Ideally, log(D(x)) = log(1) = 0
* **G(z)**: Generated image
* **D(G(z))**: Discriminator's estimate of the probability that a fake instance is real; as it is fake (generated) here, it should ideally output 0.
* **log(1 - D(G(z)))**: Ideally, log(1 - D(G(z))) = log(1 - 0) = log(1) = 0
* **m**: Size of minibath (used to average the loss over the samples)

* **Generator Loss**: The generator aims to minimize this loss, which measures how well the generator is fooling the discriminator. Alternatively, we can train G to maximize log D(G(z)) because log(1 − D(G(z))) can/will saturate.
* **Discriminator Loss**: The discriminator aims to minimize this loss, which measures how well it can differentiate between real and fake data. Represented by log(D(x)).
* **Putting it Together**: 
In training, the generator and discriminator are optimized iteratively in an adversarial manner, where the generator produces realistic data and the discriminator cannot differentiate between real and fake data effectively. The following equation represents a 2 player minimax game between the networks for value function V that takes D and G as inputs.

![image](https://github.com/shreya888/Daily-Code-Diary/assets/25200389/8ae1d5b0-1faf-4c7c-904a-e3a1f7d786e5)

To summarize the whole system:
<img src="https://github.com/shreya888/Daily-Code-Diary/assets/25200389/04c1bd64-b8cf-42c4-b923-f7038be90d72" width=500>

## Applications
GANs have found applications in various domains, including:
* Data Augmentation: Generating additional training data to improve model robustness and generalization.
* Medical Applications: GANs have been used for tasks such as privacy-preserving data generation and medical image synthesis.
* Semi-supervised Learning: Techniques like SRGAN (Super-Resolution GAN) utilize GANs for enhancing the resolution of images and other semi-supervised learning tasks.
* Style Transfer: GANs can transfer the style of one image onto another, enabling artistic transformations and image editing.

## Implementations
1. FCGAN - fully connected NN
2. DCGAN - CNN instead

## Limitations and Weakness
1. The generator and discriminator continuously adapt to each other leading to notoriously unstable training. Finding the right balance between the generator and discriminator can be challenging. Achieving convergence to a stable equilibrium can be challenging.
2. GANs are extremely sensitive to hyperparamters such as learning rates, network architectures, and batch sizes. Finding the optimal set of hyperparameters can be time-consuming and requires extensive experimentation.
3. They require large amounts of training data to learn effectively.
4. GANs can be misused for generating deepfakes, generating biased or offensive content, or infringing on privacy rights.

## References
1. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, et al. "Generative Adversarial Networks." In Advances in Neural Information Processing Systems 27, 2014. [Paper](https://arxiv.org/abs/1406.2661)
2. Martin Arjovsky, Soumith Chintala, Léon Bottou. "Wasserstein GAN." arXiv:1701.07875, 2017.
3. Christian Ledig, Lucas Theis, Ferenc Huszar, et al. "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network." arXiv:1609.04802, 2016.
4. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, et al. "Image-to-Image Translation with Conditional Adversarial Networks." arXiv:1611.07004, 2016.
5. [Deep Learning: Introduction of Generative Adversarial Networks (GANs) (TDS article) - image 1](https://towardsdatascience.com/deep-learning-introduction-of-generative-adversarial-networks-gans-ae22c4350b1f)
6. [Overview of GAN Structure (Google) - image 2](https://developers.google.com/machine-learning/gan/gan_structure)
7. [Generative Adversarial Networks (GANs)](https://www.youtube.com/playlist?list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va)
8. [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks - DCGAN paper](https://arxiv.org/abs/1511.06434)
