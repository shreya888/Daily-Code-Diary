# Generative Adversarial Networks Fundamental Concepts
Generative Adversarial Networks (GANs) are a class of machine learning techniques that consist of two networks playing an adversarial game against each other. Both the generator and discriminator start training from scratch, typically initialized with random noise, and train simultaneously. When training begins, the generator produces obviously fake data, and the discriminator quickly learns to tell that it's fake. Finally, if generator training goes well, the discriminator gets worse at telling the difference between real and fake. It starts to classify fake data as real, and its accuracy decreases.
<img src="https://github.com/shreya888/Daily-Code-Diary/assets/25200389/01b0d0ed-2b79-4e67-bf0f-a0adaf643154" width="500">

## Generator Network
* The "Counterfeiter"
* Aim: Generate synthetic data that is indistinguishable from real data by incorporating feedback from the discriminator (make the discriminator classify its output as real)
* The generated instances become negative training examples for the discriminator.
* Training Generator: Sample Random Noise input -> Produce Generator output -> Discriminator classification for gen o/p (Real/Fake) -> Calculate Loss from Discriminator classification -> Backpropagate through both the discriminator and generator to obtain gradients -> Use gradients to change only the generator weights
* Why Random Input and How?
  * NNs need some input. But what should be input to when the objective is to outputs entirely new data instances!?
  * GAN takes random noise as its input. The generator then transforms this noise into a meaningful outputs.
  * By introducing noise, GAN can produce a wide variety of data, sampling from different regions in the target distribution.
  * Experiments suggest that the distribution of the noise doesn't matter much, so we can choose something that's easy to sample from, like a uniform distribution.
  * For convenience the space from which the noise is sampled is usually of smaller dimension than the dimensionality of the output space.
* Using the Discriminator to Train the Generator
  * In traditional neural networks, weights are adjusted to minimize output error or loss.
  * But in GANs the generator is not directly connected to the loss being optimized.
  * The generator feeds into the discriminator, and the discriminator produces the output we're trying to affect.
  * The generator loss penalizes the generator for producing a sample that the discriminator network classifies as fake (penalty for failing counterfeiting). 
  * Backpropagation adjusts each weight in the right direction by calculating the weight's impact on the output — how the output would change if you changed the weight.
  * The impact of a generator weight depends on the impact of the discriminator weights it feeds into. So backpropagation starts at the output and flows back through the discriminator into the generator.
  * Preventing the discriminator to change during generator training is crutial (= hit a moving target - harder problem).

## Discriminator Network
* The "Detective"
* Aim: Distinguish between real and fake data generated by the generator through backpropagation
* Simply a classifier that penalizes the generator for producing implausible results.

### Putting it Together
In training, the generator and discriminator are optimized iteratively in an adversarial manner until a Nash equilibrium is reached, where the generator produces realistic data and the discriminator cannot differentiate between real and fake data effectively. To summarize the whole system:

<img src="https://github.com/shreya888/Daily-Code-Diary/assets/25200389/04c1bd64-b8cf-42c4-b923-f7038be90d72" width=500>

## Applications
GANs have found applications in various domains, including:
* Data Augmentation: Generating additional training data to improve model robustness and generalization.
* Medical Applications: GANs have been used for tasks such as privacy-preserving data generation and medical image synthesis.
* Semi-supervised Learning: Techniques like SRGAN (Super-Resolution GAN) utilize GANs for enhancing the resolution of images and other semi-supervised learning tasks.
* Style Transfer: GANs can transfer the style of one image onto another, enabling artistic transformations and image editing.

## References
1. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, et al. "Generative Adversarial Networks." In Advances in Neural Information Processing Systems 27, 2014.
2. Martin Arjovsky, Soumith Chintala, Léon Bottou. "Wasserstein GAN." arXiv:1701.07875, 2017.
3. Christian Ledig, Lucas Theis, Ferenc Huszar, et al. "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network." arXiv:1609.04802, 2016.
4. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, et al. "Image-to-Image Translation with Conditional Adversarial Networks." arXiv:1611.07004, 2016.
5. [Deep Learning: Introduction of Generative Adversarial Networks (GANs) (TDS article) - image 1](https://towardsdatascience.com/deep-learning-introduction-of-generative-adversarial-networks-gans-ae22c4350b1f)
6. [Overview of GAN Structure (Google) - image 2](https://developers.google.com/machine-learning/gan/gan_structure)
